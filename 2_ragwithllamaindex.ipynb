{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/david3080/llmapi/blob/main/2_ragwithllamaindex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "813NspGRKhc2"
      },
      "source": [
        "# 2.LlamaIndexを使ったRAG実装\n",
        "\n",
        "1. LlamaIndex関連ライブラリのインストール\n",
        "2. EmbeddingモデルをロードとRAG対象データのロードとインデックス化\n",
        "3. QAプロンプトとRefineプロンプトのカスタマイズ\n",
        "4. RAGを使った問い合わせを行う（4種のLLM活用）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. LlamaIndex関連ライブラリのインストール\n",
        "\n",
        "AuthropicのClaude3とOpenAIのGPT-4、GoogleのGemini、GroqのLlama3をLLMモデルとして利用し、Embedding用モデルはHuggingFaceのモデルを利用する。llama-indexは破壊的変更があったりするので、最新の[0.10](https://note.com/npaka/n/nb8acc1f63312)を指定しています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLTzvn__ldjd"
      },
      "outputs": [],
      "source": [
        "%pip install llama-index==0.10.40 llama-index-llms-anthropic llama-index-llms-openai llama-index-llms-gemini llama-index-llms-groq llama-index-embeddings-huggingface python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJTRVhkJKH4r"
      },
      "source": [
        "### 2. EmbeddingモデルをロードとRAG対象データのロードとインデックス化\n",
        "\n",
        "Embeddingには多言語E5(multilingual-e5)を利用します。RAG対象データとしては、[ポール・グレアムのエッセイの日本語訳](https://raw.githubusercontent.com/david3080/llmapi/main/data/paul_graham_essay.txt)をネットからダウンロードしてインデックス化します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQ0tqJL_mSe0"
      },
      "outputs": [],
      "source": [
        "!mkdir -p 'data/1_BasicRAG'\n",
        "!wget 'https://raw.githubusercontent.com/david3080/llmapi/main/data/paul_graham_essay.txt' -O 'data/1_BasicRAG/paul_graham_essay.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ea9GbN2poO3V"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import sys\n",
        "\n",
        "# ログレベルの設定\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG, force=True)\n",
        "\n",
        "# 多言語E5モデルをロードしてRAG対象データをEmbedding\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"intfloat/multilingual-e5-base\")\n",
        "\n",
        "from llama_index.core import (\n",
        "    Settings,\n",
        "    VectorStoreIndex,\n",
        "    SimpleDirectoryReader,\n",
        ")\n",
        "Settings.embed_model = embed_model\n",
        "Settings.chunk_size = 1024\n",
        "documents = SimpleDirectoryReader(\"./data/1_BasicRAG\").load_data()\n",
        "index = VectorStoreIndex.from_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. QAプロンプトとRefineプロンプトのカスタマイズ\n",
        "\n",
        "※ 参考: https://docs.llamaindex.ai/en/stable/examples/customization/prompts/chat_prompts/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.core.llms import ChatMessage, MessageRole\n",
        "from llama_index.core import ChatPromptTemplate\n",
        "\n",
        "qa_prompt_str = (\n",
        "    \"コンテキスト情報は下記のとおりです。\\n\"\n",
        "    \"---------------------\\n\"\n",
        "    \"{context_str}\\n\"\n",
        "    \"---------------------\\n\"\n",
        "    \"事前知識ではなく、常に提供されたコンテキスト情報を使用してクエリに回答してください。: {query_str}\\n\"\n",
        ")\n",
        "\n",
        "refine_prompt_str = (\n",
        "    \"以下の追加のコンテキストを使用して必要な場合のみ元の回答を改良します.\\n\"\n",
        "    \"------------\\n\"\n",
        "    \"{context_msg}\\n\"\n",
        "    \"------------\\n\"\n",
        "    \"新しいコンテキストを考慮して元の回答を改良して質問に対するより適切な回答をします: {query_str}. \"\n",
        "    \"コンテキストが役に立たない場合は元の回答を再度出力してください.\\n\"\n",
        "    \"元の回答: {existing_answer}\"\n",
        ")\n",
        "\n",
        "# Text QA Prompt\n",
        "chat_text_qa_msgs = [\n",
        "    ChatMessage(\n",
        "        role=MessageRole.SYSTEM,\n",
        "        content=(\n",
        "            \"コンテキストが役に立たない場合でも必ず質問に日本語で答えてください.\"\n",
        "        ),\n",
        "    ),\n",
        "    ChatMessage(role=MessageRole.USER, content=qa_prompt_str),\n",
        "]\n",
        "text_qa_template = ChatPromptTemplate(chat_text_qa_msgs)\n",
        "\n",
        "# Refine Prompt\n",
        "chat_refine_msgs = [\n",
        "    ChatMessage(\n",
        "        role=MessageRole.SYSTEM,\n",
        "        content=(\n",
        "            \"コンテキストが役に立たない場合でも必ず質問に答えてください.\"\n",
        "        ),\n",
        "    ),\n",
        "    ChatMessage(role=MessageRole.USER, content=refine_prompt_str),\n",
        "]\n",
        "refine_template = ChatPromptTemplate(chat_refine_msgs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a17Tz644KZ_P"
      },
      "source": [
        "### 4. RAGを使った問い合わせを行う（4種のLLM活用）\n",
        "\n",
        "groqのLlama3が一番レスポンスが早く、次にGPT-4、Gemini、Claude3の順番。\n",
        "\n",
        "- OpenAI https://docs.llamaindex.ai/en/stable/examples/llm/openai/\n",
        "- Anthropic https://docs.llamaindex.ai/en/stable/examples/llm/anthropic/\n",
        "- Gemini https://docs.llamaindex.ai/en/stable/examples/llm/gemini/\n",
        "- Groq https://docs.llamaindex.ai/en/stable/examples/llm/groq/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "if load_dotenv() == False:\n",
        "    from google.colab import userdata\n",
        "    os.environ[\"ANTHROPIC_API_KEY\"] = userdata.get(\"ANTHROPIC_API_KEY\")\n",
        "    os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")\n",
        "    os.environ[\"GROQ_API_KEY\"] = userdata.get(\"GROQ_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "query_str = \"著者は大学で何を学びましたか？\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIT8kqYKoaRq"
      },
      "outputs": [],
      "source": [
        "# AnthropicのClaude3を使ったクエリ\n",
        "from llama_index.llms.anthropic import Anthropic\n",
        "Settings.llm = Anthropic(model=\"claude-3-opus-20240229\")\n",
        "query_engine = index.as_query_engine(text_qa_template=text_qa_template,refine_template=refine_template)\n",
        "response = query_engine.query(query_str)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# OpenAIのGPT-4を使ったクエリ\n",
        "from llama_index.llms.openai import OpenAI\n",
        "Settings.llm = OpenAI(model=\"gpt-4\")\n",
        "query_engine = index.as_query_engine(text_qa_template=text_qa_template,refine_template=refine_template)\n",
        "response = query_engine.query(query_str)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GoogleのGeminiを使ったクエリ\n",
        "from llama_index.llms.gemini import Gemini\n",
        "Settings.llm = Gemini(model_name=\"models/gemini-pro\")\n",
        "query_engine = index.as_query_engine(text_qa_template=text_qa_template,refine_template=refine_template)\n",
        "response = query_engine.query(query_str)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GroqのLlama3を使ったクエリ\n",
        "from llama_index.llms.groq import Groq\n",
        "Settings.llm = Groq(model=\"llama3-70b-8192\")\n",
        "query_engine = index.as_query_engine(text_qa_template=text_qa_template,refine_template=refine_template)\n",
        "response = query_engine.query(query_str)\n",
        "print(response)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
